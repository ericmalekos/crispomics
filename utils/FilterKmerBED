#!/usr/bin/env python3

'''
    This script takes the output from GenerateKmerBED, and uses bedtools intersect to 
    remove uwanted kmers and retain those on the 
'''

import pandas as pd
import re
from math import floor
import argparse
import pybedtools as pyb   
from pathlib import Path


def valid_range(percentile):
    """
    Checks that the --percentile_range argument is a valid percentile
    """
    value = int(percentile)
    if value < 1 or value > 100:
            raise argparse.ArgumentTypeError(f"{value} is an invalid value, it should be in range (0, 100)")
    return value

def parse_arguments():
    parser = argparse.ArgumentParser(
        description="Filters Kmers from GenerateKmerBED."
    )

    parser.add_argument(
        "-k", "--kmers",
        type=str,
        help="KmerBED ouput of GenerateKmerBED.",
        default="./tmp/kmers.bed",
        required=False
    )

    parser.add_argument(
        "-t", "--targets",
        type=str,
        help="GTF/GFF/BED intersected with KmerBED to find targets",
        required=True
    )

    parser.add_argument(
        "-f", "--feature",
        help="Feature from third column of GTF/GFF to target. Ignored when --target is a BED file. [default: CDS]",
        type=str,
        default="CDS"
    )
    
    parser.add_argument(
        "-o", "--filterOut",
        help="BED/GTF/GFF of genomic regions to filter out.",
        type=str
    )

    # parser.add_argument(
    #     "-i", "--filterIn",
    #     help="BED/GTF/GFF of genomic regions to filter out.",
    #     type=str
    # )
    
    parser.add_argument(
        "-p", "--percentile_range",
        help="Allowable range of kmers for each transcript and feature set, e.g. \n \
            '50' returns kmers in the 5'-most half of the relevant \n \
            features for a transcript. Default settings returns kmers \n \
            anywhere in the exons for each transcript [default: 100]",
        type=valid_range,
        default="100"
    )

    return parser.parse_args()

def extract_transcript_id(attributes):
    """
    Extracts the transcript_id from the provided attributes string. The function is designed to handle both 
    GTF and GFF file formats. In case of GTF format, the transcript_id is enclosed in double quotes and followed by a semicolon.
    In case of GFF format, the transcript_id is followed by an equals sign and doesn't have the double quotes and the semicolon.

    Parameters:
    attributes (str): A string containing the attributes of a feature in a GTF/GFF file.

    Returns:
    str: The transcript_id extracted from the attributes. If no transcript_id is found, the function will print an error 
    message and terminate the program.

    Example:
    >>> extract_transcript_id('transcript_id "ENST00000456328";')
    'ENST00000456328'
    >>> extract_transcript_id('transcript_id=ENST00000456328.2;')
    'ENST00000456328.2'
    """

    match = re.search('transcript_id[ =]"?(.*?)"?;?', attributes)
    if match:
        return match.group(1)
    else:
        print("ERROR: --feature '"  + feature + "' missing 'transcript_id' attribute")
        exit(keep_kernel=True)

def process_gtf(input_file, feature = "exon", percentiles = [0,100]):
    """
    Processes a GTF/GFF file to extract and truncate specified features based on percentiles.

    Parameters:
    input_file (str): Path to the input GTF/GFF file.
    feature (str): The feature of interest. Default is 'exon'.
    percentiles (list): A list specifying the lower and upper percentile thresholds to truncate the features. Default is (0,50).

    Returns:
    DataFrame: A pandas DataFrame containing the selected and truncated features. 

    Raises:
    ValueError: If the specified feature is not found in the GTF/GFF file.

    The function reads the GTF/GFF file and selects the rows of the specified feature. 
    It then calculates the length of each feature and the cumulative length for each transcript.
    The entries where 'cumulative_percentile' exceeds 'max_percentile' are identified and 
    the 'end' or 'start' position of these entries are adjusted accordingly. 
    Finally, the function returns a DataFrame containing the selected and truncated features.

    Example:
    >>> df = process_gtf('test.gtf', 'CDS', (0, 50))
    """

    percentiles.sort()

    #print(f'\tProcessing: \t{input_file} \n\tFeature: \t{feature} \n\tPercentile range: \t{percentiles}')
    
    df = pd.read_csv(input_file, sep='\t', header=None, comment='#')
    df.columns = ["chrom", "source", "feature", "start", "end", "score", "strand", "frame", "attributes"]

    # Select the rows for the specified feature
    df = df[df["feature"] == feature]
    
    if df.empty:
        print("ERROR: --feature '"  + feature + "' not found in GTF/GFF column 3")
        exit(keep_kernel=True)
        #        raise ValueError("ERROR: --feature '"  + feature + "' not found in GTF/GFF column 3")


    df_positive = df[df["strand"] == "+"].sort_values("start")
    df_negative = df[df["strand"] == "-"].sort_values("start", ascending=False)

    # Combine the sorted dataframes
    df = pd.concat([df_positive, df_negative])

    # Extract the transcript_id from the attributes column

    df["transcript_id"] = df["attributes"].apply(extract_transcript_id)

    #print(f'\n\tNumber of {feature} entries before processing:{df.shape[0]}')
    # TODO check this
    # print(f'\n\tNumber of unique transcripts before processing:{sorted(df["transcript_id"].drop_duplicates())}')

        
    # Calculate the length of each feature
    df["length"] = abs(df["end"] - df["start"]) + 1

    # Group by the "transcript_id" and calculate the cumulative length and total length
    df["cumulative_length"] = df.groupby("transcript_id")["length"].cumsum()
    df["total_length"] = df.groupby("transcript_id")["length"].transform(sum)

    # Calculate the percentile of the cumulative length
    df["cumulative_percentile"] = df["cumulative_length"] / df["total_length"] * 100

    # Identify the entries where 'cumulative_percentile' exceeds 'max_percentile'
    overlaps = df[df["cumulative_percentile"] > percentiles[1]].groupby('transcript_id').first().reset_index()

    # Adjust the 'end' or 'start' position of overlapping entries
    for i, row in overlaps.iterrows():
        excess_percentile = row['cumulative_percentile'] - percentiles[1]
        excess_length = int(row['total_length'] * excess_percentile / 100)
        if row['strand'] == '+':
            df.loc[(df['transcript_id'] == row['transcript_id']) & (df['end'] == row['end']), 'end'] -= excess_length
        else:
            df.loc[(df['transcript_id'] == row['transcript_id']) & (df['start'] == row['start']), 'start'] += excess_length
        #print(df.to_string())
        df.loc[(df['transcript_id'] == row['transcript_id']), 'length'] = abs(df['end'] - df['start']) + 1
    # Recalculate the cumulative length and the cumulative percentile
    df["cumulative_length"] = df.groupby("transcript_id")["length"].cumsum()

    df["cumulative_percentile"] = (df["cumulative_length"] / df["total_length"] * 100).astype(int)
    
    
    df = df[df["cumulative_percentile"] <= percentiles[1]]
    
    #print(f'\n\tNumber of {feature} entries after processing:\t{df.shape[0]}')
    # TODO check this
    #print(f'\n\tNumber of unique transcripts after processing:{sorted(df["transcript_id"].drop_duplicates())}')
    
    # Drop the extra columns
    df = df.drop(columns=["length", "cumulative_length", "total_length", "cumulative_percentile", "transcript_id"])
    
    df = df.sort_values(["chrom", "start"])
    
    # Save the selected rows to a new file
    
    return df.to_csv(sep="\t", header=False, index=False, doublequote=False, quoting=3, quotechar="",  escapechar="\\")

def kmer_to_tscript(kmerIntersectGTF):
    
    df = pd.read_table(kmerIntersectGTF.fn, sep="\t", header=None)
    
    # replace 7th column with trasncript id
    df[6] = df[14].apply(extract_transcript_id)
    df.drop(columns=range(7,16), inplace=True)
    return df

def kmer_to_bed(kmerIntersectBED):
    
    df = pd.read_table(kmerIntersectBED.fn, sep="\t", header=None)
    
    # replace 7th column with chr:start_stop of BED entry
    df[6] = df[6].astype(str) + ":" + df[7].astype(str) + "_" + df[8].astype(str)
    df.drop(columns=range(7,16), inplace=True)
    return df

def bed_to_guidescan(kmerBed):
    df = pd.read_table(kmerBed.fn, usecols=[3])
    df.columns = ["id,sequence,pam,chromosome,position,sense"]
    return df.to_string(index=False, justify="left")

if __name__ == "__main__":
    args = parse_arguments()

    Path("./tmp/").mkdir(parents=True, exist_ok=True)


    # Load kmers.bed file
    kmerTargets = pyb.BedTool(args.kmers)

    # remove kmers that overlap filterOut
    if args.filterOut is not None:
        kmerTargets = kmerTargets.intersect(args.filterOut, v=True)
        
    # Determine target file type
    targetFileType = args.targets.split(".")[-1].lower()

    if targetFileType in ["gff", "gtf", "gff3", "gff2"]:
        print(f'{args.targets} is {targetFileType.upper()} format')
        trimmed = "./tmp/trimmedAnnotation." + targetFileType
        with open(trimmed, 'w') as f:
            f.writelines(process_gtf(input_file = args.targets, feature = args.feature, percentiles = [0,args.percentile_range]))
        targets = pyb.BedTool(trimmed)
        
        finalTargets = kmer_to_tscript(pyb.BedTool(kmerTargets).intersect(targets, wo=True))
        
    elif targetFileType in ["bed"]:
        print(f'{args.targets} is BED format')
        targets = pyb.BedTool(args.targets)
        
        finalTargets = kmer_to_bed(pyb.BedTool(kmerTargets).intersect(targets, wo=True))

        
    else:
        print(f'{args.targets} file extension unrecognized, for expected behavior pass GFF/GTF/BED file.\n \
        Defaulting to BED behavior')
        targets = pyb.BedTool(args.targets)
        
        finalTargets = kmer_to_bed(pyb.BedTool(kmerTargets).intersect(targets, wo=True))


    header = ["chr", "start", "stop", "id,sequence,pam,chromosome,position,sense", \
                            "context", "strand", "feature"]
    finalTargets.columns = header
    finalTargets = finalTargets.groupby(header[:6])['feature'].apply(lambda x: ','.join(set(x))).reset_index()
    finalTargets["id"] = finalTargets[header[3]].apply(lambda x: x.split(',')[0])
    finalTargets.sort_values('id', inplace=True)


    finalTargets.to_csv('./tmp/loc_filtered_guides.tsv', sep="\t", index=False)
    finalTargets[["id,sequence,pam,chromosome,position,sense"]].to_csv('./tmp/guidscan_filtered_guides.csv', sep="\t", index=False)

