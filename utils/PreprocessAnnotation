#!/usr/bin/env python3

'''

'''

import pandas as pd
import argparse
from pathlib import Path
import os

def parse_arguments():
    parser = argparse.ArgumentParser(
        description="Preprocess GTF for improved sgRNA selection."
    )

    parser.add_argument(
        "-g", "--gtf",
        type=str,
        help="GTF/GFF file to use for isoform filtering.",
        required=True
    )

    parser.add_argument(
        "-t", "--tpm_files",
        help="A list of one or more isoform quantification files produced by Salmon, \
        Kallisto or FLAIR (FLAIR outputs counts, not TPMs). The first column \
        should contain only the transcript_id and should exactly match the \
        transcript_ids in --gtf. All transcript_ids in each TPM file must be \
        common across all files and must be found in the GTF file.",
        type=str,   
        default="",
        nargs='*'    
    )

    parser.add_argument(
        "-f", "--type", 
        type=str, 
        choices=["salmon", "kallisto", "flair", "infer"], 
        help="Specify TPM input type. 'infer' guesses the input type based on the header line",
        default="infer",
    )

    parser.add_argument(
        "--mean", 
        type=int, 
        help="For a given isoform, the mean tpm/count across samples \
        must be at least this to be considered, else discard isoform.",
        default=0,
    )

    parser.add_argument(
        "--median", 
        type=int, 
        help="For a given isoform, the median tpm/count across samples \
        must be at least this to be considered, else discard isoform.",
        default=0,
    )

    parser.add_argument(
        "--min", 
        type=int, 
        help="For a given isoform, each sample must have at least \
        this tpm/count to be considered, else discard isoform.",
        default=0,
    )

    parser.add_argument(
        "--max", 
        type=int, 
        help="For a given isoform, at least one sample must have at least \
        this tpm/count to be considered, else discard isoform.",
        default=0,
    )

    parser.add_argument(
        "-n", "--top_n", 
        type=int, 
        help="For a given gene, rank all isoforms by median_tpm, keep the \
        top_n ranked isoforms and discard the rest. '-1' to keep all isoforms.",
        default=-1,
    )

    parser.add_argument(
        "-c", "--top_n_column", 
        type=str, 
        choices=["median", "mean", "min", "max"], 
        help="The metric by which to rank and filter isoforms. \
            Used with '-n' to select the most higlhy expressed transcripts. [Default:median]",
        default="median",
    )

    parser.add_argument(
        "-m", "--model", 
        type=str, 
        choices=["metagene", "consensus", "none", "both"], 
        help="Whether to collapse gene isoforms using the 'metagene' or 'consensus' model.",
        default="none",
    )   

    parser.add_argument(
        "-w ", "--tss_window", 
        nargs=2, 
        type=int, 
        default=None,  # Setting the default value to a list of two integers
        help="Pass two, space-separated, integers to specifiy the bp window around the TSS \
            as '<upstream>' '<downstream>'. e.g. --tss_window 250 150"
    )

    parser.add_argument(
        "-x ", "--tx_to_gene", 
        type=str, 
        default=None,
        help="A TSV with transcript IDs in the first column and Gene IDs in the second. \
        The transcript IDs must match the first column entries of the --quant_files. \
        If this is not provided it will be deduced from the GTF/GFF3 and saved as \
        './annotations/intermediateFiles/tx2_gene.tsv'." 
    )


    return parser.parse_args()


def parse_input(input_file):
    genes = {}

    with open(input_file, 'r') as f:
        for line in f:
            if line.startswith("#"):
                continue

            fields, attributes = parse_line(line)
            if not fields or not attributes:
                continue

            gene_id = attributes.get('gene_id', None)
            transcript_id = attributes.get('transcript_id', None)
            chromosome = fields[0]
            start, end = int(fields[3]), int(fields[4])
            feature_type = fields[2]
            strand = fields[6]

            if gene_id not in genes:
                genes[gene_id] = {'exons': {}, 'CDS_coords': {}, 'CDS': set(), 'strand': strand, 'chromosome': chromosome}

            if feature_type == 'exon':
                if transcript_id not in genes[gene_id]['exons']:
                    genes[gene_id]['exons'][transcript_id] = []
                genes[gene_id]['exons'][transcript_id].append((start, end))
            elif feature_type == 'CDS':
                genes[gene_id]['CDS'].add(transcript_id)
                if transcript_id not in genes[gene_id]['CDS_coords']:
                    genes[gene_id]['CDS_coords'][transcript_id] = []
                genes[gene_id]['CDS_coords'][transcript_id].append((start, end))

    return genes

def parse_line(line):
    fields = line.strip().split('\t')
    attributes = {}

    for attr in fields[8].split(';'):
        attr = attr.strip()
        if not attr:
            continue

        # Checking if it's more like a GTF (space delimiter) or GFF (equal delimiter)
        if ' ' in attr:
            key, value = attr.split(' ', 1)
            attributes[key] = value.strip('"')
        elif '=' in attr:
            key, value = attr.split('=', 1)
            attributes[key] = value

    return fields, attributes

def extract_transcript_gene_relationship(input_file):
    """
    Extracts a mapping between transcript_ids and gene_ids from the GTF/GFF file.

    Parameters:
    - input_file: path to the GTF/GFF file

    Returns:
    - A dictionary with transcript_ids as keys and gene_ids as values.
    """
    relationship = {}
    with open(input_file, 'r') as f:
        for line in f:
            if line.startswith("#"):
                continue

            _, attributes = parse_line(line)
            gene_id = attributes.get('gene_id', None)
            transcript_id = attributes.get('transcript_id', None)
            if transcript_id and gene_id:
                relationship[transcript_id] = gene_id
    return relationship

def write_utr(exon, start, end, cds_start, cds_end, strand, attributes):
    utr_str = ""

    if strand == '+':
        if start < cds_start:
            utr5_fields = exon.copy()
            utr5_fields[2] = '5UTR'
            utr5_fields[4] = str(min(end, cds_start - 1))
            utr5_fields[8] = attributes
            utr_str += "\t".join(utr5_fields) + "\n"
        
        if end > cds_end:
            utr3_fields = exon.copy()
            utr3_fields[2] = '3UTR'
            utr3_fields[3] = str(max(start, cds_end + 1))
            utr3_fields[8] = attributes
            utr_str += "\t".join(utr3_fields) + "\n"
    else:
        if end > cds_end:
            utr5_fields = exon.copy()
            utr5_fields[2] = '5UTR'
            utr5_fields[3] = str(max(start, cds_end + 1))
            utr5_fields[8] = attributes
            utr_str += "\t".join(utr5_fields) + "\n"
        
        if start < cds_start:
            utr3_fields = exon.copy()
            utr3_fields[2] = '3UTR'
            utr3_fields[4] = str(min(end, cds_start - 1))
            utr3_fields[8] = attributes
            utr_str += "\t".join(utr3_fields) + "\n"

    return utr_str

def generate_output_str(genes, consensus_exons, consensus_CDS, label = "consensus" ):
    output_str = ''

    for gene_id, exons in consensus_exons.items():
        # Define CDS bounds based on consensus_CDS
        cds_list = sorted(list(consensus_CDS.get(gene_id, [])))
        if cds_list:
            cds_start, _ = cds_list[0]
            _, cds_end = cds_list[-1]
        else:
            cds_start, cds_end = None, None

        if exons:
            exons = sorted(list(exons), key=lambda x: x[0])
            transcript_start = exons[0][0]
            transcript_end = exons[-1][1]
            transcript_line = [
                genes[gene_id]['chromosome'], label, 'transcript', str(transcript_start), str(transcript_end), '.', 
                genes[gene_id]['strand'], '.', f'gene_id "{gene_id}"; transcript_id "{gene_id}.{label}"'
            ]
            output_str += "\t".join(transcript_line) + "\n"

        for exon_tuple in exons:
            start, end = exon_tuple
            exon_line = [
                genes[gene_id]['chromosome'], label, 'exon', str(start), str(end), '.', 
                genes[gene_id]['strand'], '.', f'gene_id "{gene_id}"; transcript_id "{gene_id}.{label}"'
            ]
            output_str += "\t".join(exon_line) + "\n"
            if cds_start and cds_end:  # Only write UTRs if there is a consensus CDS
                attributes = f'gene_id "{gene_id}"; transcript_id "{gene_id}.{label}"'
                output_str += write_utr(exon_line, start, end, cds_start, cds_end, genes[gene_id]['strand'], attributes)

        for cds_tuple in consensus_CDS.get(gene_id, []):
            start, end = cds_tuple
            cds_line = [
                genes[gene_id]['chromosome'], label, 'CDS', str(start), str(end), '.', 
                genes[gene_id]['strand'], '.', f'gene_id "{gene_id}"; transcript_id "{gene_id}.{label}"'
            ]
            output_str += "\t".join(cds_line) + "\n"

    return output_str

def create_metagene_model(input_file):
    genes = parse_input(input_file)
    
    # Phase 2: Combine all exons for metagene model
    metagene_exons = {}
    metagene_CDS = {}

    for gene_id, data in genes.items():
 
        cds_transcripts = get_transcripts_with_cds(data['CDS'])

        all_transcripts = set(data['exons'].keys())

        if len(all_transcripts) == 0:
            print("\t\tWARNING:\tNo exon found for " + gene_id + ", skipping this gene. Check GTF.")
            continue

        use_transcripts = cds_transcripts if cds_transcripts else all_transcripts

        combined_exons = []

        for transcript in use_transcripts:
            combined_exons.extend(data['exons'][transcript])
              
        combined_exons.sort(key=lambda x: x[0])
        
        merged_exons = []
        current_exon = combined_exons[0]

        for exon in combined_exons[1:]:
            if exon[0] <= current_exon[1] + 1:
                current_exon = (current_exon[0], max(current_exon[1], exon[1]))
            else:
                merged_exons.append(current_exon)
                current_exon = exon
        merged_exons.append(current_exon)

        metagene_exons[gene_id] = merged_exons

        # If CDS transcripts are present, combine and merge them similarly to exons
        if cds_transcripts:
            combined_cds = []
            for transcript in cds_transcripts:
                combined_cds.extend(data['CDS_coords'][transcript])
            combined_cds.sort(key=lambda x: x[0])

            merged_cds = []
            current_cds = combined_cds[0]
            for cds in combined_cds[1:]:
                if cds[0] <= current_cds[1] + 1:
                    current_cds = (current_cds[0], max(current_cds[1], cds[1]))
                else:
                    merged_cds.append(current_cds)
                    current_cds = cds
            merged_cds.append(current_cds)
            metagene_CDS[gene_id] = merged_cds

    # Phase 3: save output (remains mostly unchanged, but work with `metagene_exons` and `metagene_CDS` dicts)
    output_str = generate_output_str(genes, metagene_exons, metagene_CDS, label='metagene')

    return output_str

def is_inside(region1, region2):
    """
    Check if region1 is inside region2.
    """
    return region2[0] <= region1[0] and region2[1] >= region1[1]

def get_max_start_min_end(regions):
    if not regions:
        return None
    max_start = max([start for start, _ in regions])
    min_end = min([end for _, end in regions])
    
    if max_start > min_end:
        return None
    
    return (max_start, min_end)

def get_transcripts_with_cds(cds_list):
    return set(cds_list)

def overlapping_regions_for_transcripts(region, transcripts, all_regions):
    overlapping_regions = [region]
    for transcript in transcripts:
        overlaps_current_transcript = False
        for t_region in all_regions.get(transcript, []):
            if t_region[1] >= region[0] and t_region[0] <= region[1]:
                overlapping_regions.append(t_region)
                overlaps_current_transcript = True
        if not overlaps_current_transcript:
            return None
    return get_max_start_min_end(overlapping_regions)

def create_constitutive_model(input_file):
    genes = parse_input(input_file)

    # Phase 2: Find consensus exons and CDS
    consensus_exons = {}
    consensus_CDS = {}

    for gene_id, data in genes.items():
        cds_transcripts = get_transcripts_with_cds(data['CDS'])
        all_transcripts = set(data['exons'].keys())
        
        if not cds_transcripts:  # If no transcripts have CDS, continue to next gene
            continue

        # Process exons
        for transcript, exons in data['exons'].items():
            for exon in exons:
                consensus = overlapping_regions_for_transcripts(exon, all_transcripts, data['exons'])
                if consensus:
                    consensus_exons[gene_id] = consensus_exons.get(gene_id, set())
                    consensus_exons[gene_id].add(consensus)

        # Process CDS
        for transcript, cds_coords in data['CDS_coords'].items():
            for cds in cds_coords:
                cds_consensus = overlapping_regions_for_transcripts(cds, cds_transcripts, data['CDS_coords'])
                if cds_consensus:
                    consensus_CDS[gene_id] = consensus_CDS.get(gene_id, set())
                    consensus_CDS[gene_id].add(cds_consensus)

    # Phase 3: save output
    output_str = generate_output_str(genes, consensus_exons, consensus_CDS)
    genes_without_consensus = set(genes.keys()) - set(consensus_exons.keys())

    return output_str, genes_without_consensus
        

def calculate_statistics(df):
    tpm_columns = [col for col in df.columns if 'TPM_' in col or 'COUNTS_' in col]
    
    df['tscript_min'] = df[tpm_columns].min(axis=1)
    df['tscript_max'] = df[tpm_columns].max(axis=1)
    df['tscript_median'] = df[tpm_columns].median(axis=1)
    df['tscript_mean'] = df[tpm_columns].mean(axis=1)
    return df.round(2)

def process_kallisto(files):
    dfs = []
    for file in files:
        data = pd.read_csv(file, sep='\t')
        # Renaming 'target_id' column to 'transcript_id'
        dfs.append(data[['target_id', 'tpm']].rename(columns={'target_id': 'transcript_id', 'tpm': f'TPM_{file}'}))

    if len(dfs) > 1:
        return process_dataframes(dfs)
    else:
        return calculate_statistics(dfs[0])

def process_salmon(files):
    dfs = []
    for file in files:
        data = pd.read_csv(file, sep='\t')
        # Renaming 'Name' column to 'transcript_id'
        dfs.append(data[['Name', 'TPM']].rename(columns={'Name': 'transcript_id', 'TPM': f'TPM_{file}'}))
    
    if len(dfs) > 1:
        return process_dataframes(dfs)
    else:
        return calculate_statistics(dfs[0])

def process_flair(files):
    dfs = []
    for file in files:
        data = pd.read_csv(file, sep='\t')
        # Renaming 'target_id' column to 'transcript_id'
        data.columns = [data.columns[0]] + [f'COUNTS_{col}' for col in data.columns[1:]]
        data.iloc[:, 0] = data.iloc[:, 0].str.split('_').str[0]
        dfs.append(data.rename(columns={'id': 'transcript_id'}))

    if len(dfs) > 1:
        return process_dataframes(dfs)
    else:
        return calculate_statistics(dfs[0])
    
def process_dataframes(dfs):
    merged_df = dfs[0]
    for df in dfs[1:]:
        merged_df = pd.merge(merged_df, df, on=df.columns[0], how='outer')
    return calculate_statistics(merged_df)

def infer_file_type_from_first_line(path):
    print("\n\tInferring file type from header line\n")
    try:
        with open(path, 'r') as file:
            first_line = file.readline().strip()
        if first_line.startswith('target_id\tlength\teff_length\test_counts\ttpm'):
            print("\t\t" + path + " is a Kallisto file")
            return 'kallisto'
        elif first_line.startswith('Name\tLength\tEffectiveLength\tTPM\tNumReads'):
            print("\t\t" + path + " is a Salmon file")
            return 'salmon'
        
        elif first_line.startswith('id\t'):
            print("\t\t" + path + " is a FLAIR file")
            return 'flair'
        else:
            print("\n\t\tCould not determine file type for " + path)
            print("\t\tCheck file header")
            return None
    except Exception as e:
        print(f"\n\tCould not read file {path} to infer file type: {e}")
        return None

def process_files(paths, filetype="infer"):
    if not isinstance(paths, list):
        paths = [paths]

    kallisto_files = []
    salmon_files = []
    flair_files = []

    for path in paths:
        if os.path.isfile(path):
            if filetype == "infer":
                filetype = infer_file_type_from_first_line(path)
            
            #print("\n\tProcessing files as " + filetype)
            if filetype == "kallisto":
                kallisto_files.append(path)
            elif filetype == "salmon":
                salmon_files.append(path)
            elif filetype == "flair":
                flair_files.append(path)
            else:
                raise ValueError(f"Invalid file type for path: {path}")
        else:
            raise ValueError(f"Path is not a file: {path}")

    if sum([bool(kallisto_files), bool(salmon_files), bool(flair_files)]) == 0:
        raise ValueError("Unable to infer filetype")
    if sum([bool(kallisto_files), bool(salmon_files), bool(flair_files)]) > 1:
        raise ValueError("Multiple filetypes passed. Please pass only one of Salmon/Kallisto/FLAIR")
  
    if kallisto_files:
        return process_kallisto(kallisto_files)
    elif salmon_files:
        return process_salmon(salmon_files)
    elif flair_files:
        return process_flair(flair_files)
    else:
        raise ValueError("No valid files found.")

def filter_dataframe(df, **filters):

    print(f'\n\tInitial unique transcripts:\t\t\t{    df["transcript_id"].nunique()}')

    for column, threshold in filters.items():
        if column in ["tscript_min", "tscript_max", "tscript_median", "tscript_mean"]:
            df = df[df[column] >= threshold]
    
    print(f'\tTranscripts after filtering by expression:\t{df["transcript_id"].nunique()}')

    return df

def add_gene_ids_and_subset(df, relationship, col = 'median', n=-1):
    """
    Adds gene IDs to the dataframe and selects the top n transcripts based on a specified column.

    Parameters:
    - df: the dataframe containing transcript data.
    - relationship: a dictionary with transcript_ids as keys and gene_ids as values.
    - n: the number of transcripts to select based on the specified column.
    - column_name: the name of the column to be used for sorting.
    - handle_missing: how to handle transcript IDs not found in relationship ('drop', 'placeholder', 'warn').

    Returns:
    - A modified dataframe containing the gene_id column and filtered rows.
    """
    
    # Map transcript_id to gene_id using the relationship dictionary
    df['gene_id'] = df['transcript_id'].map(relationship)

        
    #print(f'\tInitial unique genes: {df["gene_id"].nunique()}')


    df = df[['gene_id'] + [col for col in df if col != 'gene_id']]


    
    # check for duplicates
    # duplicated_entries = df[df['gene_id'].duplicated(keep=False)]    
    
    # Handle missing gene_ids based on the provided method
    missing_transcripts = df[df['gene_id'].isna()]['transcript_id'].tolist()
    if missing_transcripts:
        print('\n\tWarning: Transcripts not found in GTF/GFF:\n\n\t' + '\n\t'.join(missing_transcripts[:10]) + '\n\t...\n\tand ' + \
                                                                         str(len(missing_transcripts) - 10) + ' more' )
        print('\tGENE LEVEL FILTERING OF TOP TRANSCRIPTS WILL BE IGNORED')
        print('\tCHECK THAT THE SAME ANNOTATION WAS USED FOR QUNATIFICATION AND CURRENT PROCESSING')
        print('\tCHECK THAT THE FIRST COLUMN OF THE TPM QUANTIFICATION FILES CONTAINS ONLY THE TRANSCRIPT ID')
        
        return df.drop('gene_id', axis=1)
            
    # Sort by the specified column in descending order and then group by gene_id 
    # to select the top n transcripts

    print(f'\tRetaining top {n} transcripts per gene')

    col = 'tscript_' + col

    def get_top_n_or_all(group: pd.DataFrame) -> pd.DataFrame:
        return group.nlargest(n, col)

    if n >= 0:
        df = df.groupby('gene_id', group_keys=False).apply(get_top_n_or_all)

    print(f'\tFinal unique genes:\t\t{df["gene_id"].nunique()}')
    print(f'\tFinal unique transcripts:\t{df["transcript_id"].nunique()}')

    return df


def filter_gtf_by_transcript_ids(input_file, transcript_ids):
    """
    Filters the GTF/GFF file based on the provided set of transcript IDs.

    Parameters:
    - input_file: path to the GTF/GFF file
    - transcript_ids: a set containing transcript IDs to be filtered for.

    Returns:
    - A list containing lines from the GTF/GFF file that match the given transcript IDs.
    """
    filtered_lines = []

    with open(input_file, 'r') as f:
        for line in f:
            if line.startswith("#"):
                continue

            _, attributes = parse_line(line)
            transcript_id = attributes.get('transcript_id', None)


            if transcript_id and transcript_id in transcript_ids:
                filtered_lines.append(line)

    return filtered_lines

def gtf_to_tss_bed(input_gtf, upstream=500, downstream=500):
    """
    Converts GTF file to a BED file with TSS positions for each transcript.

    Parameters:
    - input_gtf: Path to the input GTF file.
    - output_bed: Path to the output BED file.
    - upstream: The number of bases upstream of the TSS to include in the BED entry.
    - downstream: The number of bases downstream of the TSS to include in the BED entry.
    """

    tss_entries = []

    with open(input_gtf, 'r') as gtf:
        for line in gtf:
            if not line.startswith("#"):  # skip header lines
                fields, attributes = parse_line(line)

                if fields[2] == 'transcript':
                    chrom = fields[0]
                    start = int(fields[3]) - 1  # 0-based start for BED format
                    end = int(fields[4])  # 1-based end for BED format
                    strand = fields[6]
                    
                    # Ensure the transcript_id attribute is present before proceeding
                    if 'transcript_id' not in attributes:
                        continue
                    
                    # Determine TSS and create a window around it based on strand orientation
                    tss_start = start if strand == '+' else end
                    window_start = max(0, tss_start - upstream)
                    window_end = tss_start + downstream
                    
                    # Create BED entry
                    bed_entry = [chrom, str(window_start), str(window_end), attributes['transcript_id'], '0', strand,
                                 str(tss_start), str(tss_start+1)]
                    tss_entries.append('\t'.join(bed_entry))
    
    if not tss_entries:
        print('\tTSS could not be inferred.\n\tCheck that ' + input_gtf + ' has \'transcript\' attributes in the third column.')

    return tss_entries

def save_tss_bed(args, GTF_path):

    if args.model != "none":
        print('\n\t\tWARNING: TSSs are found after "metagene" or "consensus" model creation')
        print('\t\tThese TSSs should be carefully examined in a genome browser because alternate')
        print('\t\tfirst exon usage can make these results extremely non-optimal')

    output_bed = gtf_to_tss_bed(GTF_path, upstream=args.tss_window[0], downstream=args.tss_window[1])

    TSS_bed = './annotations/TSS_' + '.'.join(GTF_file.split('.')[:-1]) + '.bed'


    print('\n\tSaving TSS:\t' + TSS_bed + '\n')
    with open(TSS_bed, 'w') as f:
        for entry in output_bed:
            f.write(entry + '\n')



if __name__ == "__main__":

    args = parse_arguments()

    Path("./annotations/intermediateFiles/").mkdir(parents=True, exist_ok=True)

    GTF_path = args.gtf
    GTF_file = args.gtf.split('/')[-1]

    print(GTF_path,GTF_file)

    print('\n')

    if args.tpm_files:
        
        print('\tProcessing isoform quantification files')
        
        print('\n\tRemoving transcripts below threshold')

        transcript_df = filter_dataframe(process_files(args.tpm_files), \
                                        tscript_max = args.max, \
                                        tscript_min = args.min, \
                                        tscript_mean = args.mean, \
                                        tscript_median = args.median )



        tx_to_gene = {}
        
        if not args.tx_to_gene:
            print('\n\tGenerating transcript-gene relationships')
            tx_to_gene = extract_transcript_gene_relationship(GTF_path)
            tx2gene = './annotations/intermediateFiles/tx2_gene.tsv'
            print('\n\tSaving transcript-gene relationships to:\t' + tx2gene)
            with open("./annotations/intermediateFiles/tx2_gene.tsv", 'w') as f:
                for key, value in tx_to_gene.items():
                    f.write(f"{key}\t{value}\n")
        else:
            print('\n\tImporting transcript-gene relationships from ' + args.tx_to_gene)
            with open(args.tx_to_gene, 'r') as f:
                for line in f:
                    key, value = line.strip().split('\t')
                    tx_to_gene[key] = value

        result_df = add_gene_ids_and_subset(transcript_df, relationship=tx_to_gene, col = args.top_n_column, n = args.top_n)
        result_df_out = './annotations/intermediateFiles/filtered_' + '.'.join(GTF_file.split('.')[:-1]) + '.tsv'

        print('\tSaving quantification file to:\t\t' + result_df_out)
        result_df.to_csv(result_df_out, sep="\t", index=False)


        #these are the transcript ids that should be extracted from GTF/GFF
        transcript_ids = set(result_df['transcript_id'].tolist())
        
        #transcript_ids = extract_transcript_ids_from_dataframe(result_df)
        filtered_gtf_lines = filter_gtf_by_transcript_ids(input_file = GTF_path, transcript_ids = transcript_ids)

        # for i in list(filtered_gtf_lines):
        #     if "NM_019484.4" in i:
        #         print(i)

        GTF_file = 'filtered_' + '.'.join(GTF_file.split('.')[:-1]) + '.gtf'
        GTF_path = './annotations/' + GTF_file
        print('\tSaving transcript filtered GTF to:\t' + GTF_path)
        with open(GTF_path, 'w') as f:
            for line in filtered_gtf_lines:
                f.write(line)

    if args.model != "none":
        cur_GTF_file = GTF_file
        cur_GTF_path = GTF_path
        if args.model == "metagene" or args.model == "both":
            output_str = create_metagene_model(cur_GTF_path)

            GTF_file = 'meta_' + '.'.join(cur_GTF_file.split('.')[:-1]) + '.gtf'
            GTF_path = './annotations/' + GTF_file
            print('\n\tSaving metagene GTF to:\t\t' + GTF_path)
            with open(GTF_path, 'w') as f:
                f.write(output_str)
            
            if args.tss_window:
                save_tss_bed(args, GTF_path = GTF_path)

        if args.model == "consensus" or args.model == "both":

            output_str, genes_without_consensus = create_constitutive_model(cur_GTF_path)

            GTF_file = 'consensus_' + '.'.join(cur_GTF_file.split('.')[:-1]) + '.gtf'
            GTF_path = './annotations/' + GTF_file

            print('\tSaving consensus GTF to: ' + GTF_path)
            with open(GTF_path, 'w') as f:
                f.write(output_str)

            genes_without_consensus_out = './annotations/genes_without_consensus_exons.txt'
            print('\n\tA consensus model could not be generated for ' + str(len(genes_without_consensus)) + ' genes')
            print('\tIf this number is large, consider filtering by TPM expression more strictly or using a more conservative GTF')
            print('\tIf this number is small, consider manually removing problematic transcripts from the quantification TSVs and rerunning this module')
            print('\tSaving genes for which there is no consensus model to:\t' + genes_without_consensus_out)
            with open(genes_without_consensus_out, 'w') as f:
                f.writelines(gene_id + "\n" for gene_id in genes_without_consensus)

            if args.tss_window:
                save_tss_bed(args, GTF_path = GTF_path)

    elif args.model != "none" and args.tss_window:
        save_tss_bed(args, GTF_path = GTF_path)

    print('\n')

    