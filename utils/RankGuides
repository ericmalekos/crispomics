#!/usr/bin/env python3

'''
    This script takes the output from FilterKmerBED and scores it
    with Ruleset3 (cleavage efficieny, sequence-based) and Guidescan2 (off-target). 
'''

import pandas as pd
from pathlib import Path
import argparse
import pybedtools as pyb
import re

def restricted_float(x):
    x = float(x)
    if x < 0.0 or x > 1.0:
        raise argparse.ArgumentTypeError(f"{x} not in range [0.0, 1.0]")
    return x

def parse_arguments():
    parser = argparse.ArgumentParser(
        description="Rank Kmers from ScoreKmers."
    )

    parser.add_argument(
        "-k", "--scoredGuides",
        type=str,
        help="AllScoredGuides.tsv ouput from ScoreKmers.",
        required=True
    )

    parser.add_argument(
        "-t", "--targets",
        type=str,
        help="BED/GTF/GFF used to select final guides per target. \
            For GTF/GFF, targets are transcripts. For BED, targets are each entry. \
            Use '--number_of_targets' to set the number of guides chosed for each target.",
        required=True
    )

    parser.add_argument(
        "-f", "--feature",
        type=str,
        help="If GTF/GFF passed, use this feature for processing e.g. 'exon', 'CDS', '5UTR', etc. \
            The feature should match entries in the third column of the GTF/GFF.",
        default='exon'
    )

    parser.add_argument(
        "-p", "--percentile_range",
        help="Allowable range of guide for each transcript and feature set, e.g. \n \
            '0.5' returns kmers in the 5'-most half of the relevant \n \
            features for a transcript. Default settings returns guides \n \
            anywhere in the exons for each transcript [default: a]",
        type=restricted_float,
        default=1
    )

    parser.add_argument(
        "-n", "--number_of_targets",
        type=int,
        help="Number of guides returned per target.",
        default=6
    )

    parser.add_argument(
        "--rs3_weight", 
        type=restricted_float, 
        default=0.5, 
        help="Weight for cleavage scoring")

    parser.add_argument(
        "--gscan_weight", 
        type=restricted_float, 
        default=0.5, 
        help="Weight for guidescan")

    parser.add_argument(
        "--kmers_per_tscript", 
        type=int, 
        default=10, 
        help="Kmers per transcript")

    parser.add_argument(
        "--min_specificity", 
        type=restricted_float, 
        default=0.5, 
        help="Minimum guidescan specificity score from [0,1]")

    parser.add_argument(
        "--min_rs3", 
        type=float, 
        default=0.5, 
        help="Minimum cleavage score after scaling \
            scores to [0,1]. Used with --rs3_column")

    parser.add_argument(
        "--rs3_column", 
        type=str, 
        choices=["rs3_score_norm", "rs3_cdf"], 
        default="rs3_score_norm", 
        help="Minimum cleavage score after scaling \
            scores to [0,1]. Either 'rs3_score_norm' or 'rs3_cdf' column are allowed. \
            Either should provide similar results as their rank ordering is the same.")

    return parser.parse_args()


def extract_transcript_id(attributes):
    """
    Extracts the transcript_id from the provided attributes string. The function is designed to handle both 
    GTF and GFF file formats. In case of GTF format, the transcript_id is enclosed in double quotes and followed by a semicolon.
    In case of GFF format, the transcript_id is followed by an equals sign and doesn't have the double quotes and the semicolon.

    Parameters:
    attributes (str): A string containing the attributes of a feature in a GTF/GFF file.

    Returns:
    str: The transcript_id extracted from the attributes. If no transcript_id is found, the function will print an error 
    message and terminate the program.

    Example:
    >>> extract_transcript_id('transcript_id "ENST00000456328";')
    'ENST00000456328'
    >>> extract_transcript_id('transcript_id=ENST00000456328.2;')
    'ENST00000456328.2'
    """

    match = re.search(r'transcript_id[= ]"?([^";]*)"?', attributes)
    #match = re.search('transcript_id[ =]"?(.*?)"?;?', attributes)
    #match = re.search(r'gene_id[= ]"?([^";]*)"?', attributes)
    if match:
        return match.group(1)
    else:
        print("ERROR: --feature '"  + feature + "' missing 'transcript_id' attribute")
        exit(keep_kernel=True)
        
def extract_ids(attributes):
    
    #print(attributes)
    
    transcript_match = re.search(r'transcript_id[= ]"?([^";]*)"?', attributes)

    gene_match = re.search(r'gene_id[= ]"?([^";]*)"?', attributes)

    #print(transcript_match.group(1))
    
    transcript_id = transcript_match.group(1) if transcript_match else None
    gene_id = gene_match.group(1) if gene_match else None
    
    #print(transcript_id, gene_id)
    return transcript_id, gene_id

def process_gtf(input_file, feature = "exon", percentiles = [0,100]):
    """
    Processes a GTF/GFF file to extract and truncate specified features based on percentiles.

    Parameters:
    input_file (str): Path to the input GTF/GFF file.
    feature (str): The feature of interest. Default is 'exon'.
    percentiles (list): A list specifying the lower and upper percentile thresholds to truncate the features. Default is (0,50).

    Returns:
    DataFrame: A pandas DataFrame containing the selected and truncated features. 

    Raises:
    ValueError: If the specified feature is not found in the GTF/GFF file.

    The function reads the GTF/GFF file and selects the rows of the specified feature. 
    It then calculates the length of each feature and the cumulative length for each transcript.
    The entries where 'cumulative_percentile' exceeds 'max_percentile' are identified and 
    the 'end' or 'start' position of these entries are adjusted accordingly. 
    Finally, the function returns a DataFrame containing the selected and truncated features.

    Example:
    >>> df = process_gtf('test.gtf', 'CDS', (0, 50))
    """


    percentiles.sort()

    print(f'\tProcessing: \t{input_file} \n\tFeature: \t{feature} \n\tPercentile range: \t{percentiles}')
    
    df = pd.read_csv(input_file, sep='\t', header=None, comment='#')
    df.columns = ["chrom", "source", "feature", "start", "end", "score", "strand", "frame", "attributes"]

    # Select the rows for the specified feature
    df = df[df["feature"] == feature]
    
    if df.empty:
        print("ERROR: --feature '"  + feature + "' not found in GTF/GFF column 3")
        exit(keep_kernel=True)
        #        raise ValueError("ERROR: --feature '"  + feature + "' not found in GTF/GFF column 3")


    df_positive = df[df["strand"] == "+"].sort_values("start")
    df_negative = df[df["strand"] == "-"].sort_values("start", ascending=False)

    # Combine the sorted dataframes
    df = pd.concat([df_positive, df_negative])

    # Extract the transcript_id from the attributes column

    #print(df["attributes"].apply(extract_ids))
    #df["transcript_id"], df["gene_id"] = df["attributes"].apply(extract_ids)
    
    df[["transcript_id", "gene_id"]] = df["attributes"].apply(extract_ids).apply(pd.Series)


    #df["transcript_id"] = df["attributes"].apply(extract_transcript_id)
    
    print(f'\n\tNumber of {feature} entries before processing:{df.shape[0]}')
    # TODO check this
    # print(f'\n\tNumber of unique transcripts before processing:{sorted(df["transcript_id"].drop_duplicates())}')

        
    # Calculate the length of each feature
    df["length"] = abs(df["end"] - df["start"]) + 1

    # Group by the "transcript_id" and calculate the cumulative length and total length
    df["cumulative_length"] = df.groupby("transcript_id")["length"].cumsum()
    df["total_length"] = df.groupby("transcript_id")["length"].transform(sum)

    # Calculate the percentile of the cumulative length
    df["cumulative_percentile"] = df["cumulative_length"] / df["total_length"] * 100

    # Identify the entries where 'cumulative_percentile' exceeds 'max_percentile'
    overlaps = df[df["cumulative_percentile"] > percentiles[1]].groupby('transcript_id').first().reset_index()

    # Adjust the 'end' or 'start' position of overlapping entries
    for i, row in overlaps.iterrows():
        excess_percentile = row['cumulative_percentile'] - percentiles[1]
        excess_length = int(row['total_length'] * excess_percentile / 100)
        if row['strand'] == '+':
            df.loc[(df['transcript_id'] == row['transcript_id']) & (df['end'] == row['end']), 'end'] -= excess_length
        else:
            df.loc[(df['transcript_id'] == row['transcript_id']) & (df['start'] == row['start']), 'start'] += excess_length
        #print(df.to_string())
        df.loc[(df['transcript_id'] == row['transcript_id']), 'length'] = abs(df['end'] - df['start']) + 1
    # Recalculate the cumulative length and the cumulative percentile
    df["cumulative_length"] = df.groupby("transcript_id")["length"].cumsum()

    df["cumulative_percentile"] = (df["cumulative_length"] / df["total_length"] * 100).astype(int)
    
    
    df = df[df["cumulative_percentile"] <= percentiles[1]]
    
    print(f'\n\tNumber of {feature} entries after processing:\t{df.shape[0]}')
    # TODO check this
    #print(f'\n\tNumber of unique transcripts after processing:{sorted(df["transcript_id"].drop_duplicates())}')
    
    # Drop the extra columns
    df = df.drop(columns=["length", "cumulative_length", "total_length", "cumulative_percentile", "transcript_id", "gene_id"])
    
    df = df.sort_values(["chrom", "start"])
    
    # Save the selected rows to a new file
    
    return df.to_csv(sep="\t", header=False, index=False, doublequote=False, quoting=3, quotechar="",  escapechar="\\")

def kmer_to_tscript(kmerIntersectGTF):
    
    df = pd.read_table(kmerIntersectGTF.fn, sep="\t", header=None)

    # replace 7th column with transcript id
    #df[6] = df[19].apply(extract_transcript_id)
    df[['transcript_id', 'gene_id']] = df.iloc[:, 19].apply(extract_ids).apply(pd.Series)

    df.drop(columns=range(11,21), inplace=True)
    return df

def kmer_to_bed(kmerIntersectBED):
    
    df = pd.read_table(kmerIntersectBED.fn, sep="\t", header=None)
    
    # replace 7th column with chr:start_stop of BED entry
    df[6] = df[6].astype(str) + ":" + df[7].astype(str) + "_" + df[8].astype(str)
    df.drop(columns=range(7,16), inplace=True)
    return df   

min_distance = 1000  # Adjust based on your requirement

def maximize_spacing(group):
    # Sorting the kmers by score (adjust as per your score column)
    group = group.sort_values(by='rs3_score_norm', ascending=False)
    
    # List to store selected kmers
    selected_kmers = [group.iloc[0]]
    
    # Loop to iteratively add kmers maintaining minimum distance
    for i, row in group.iterrows():
        if all(abs(row['start'] - kmer['start']) >= min_distance for kmer in selected_kmers):
            selected_kmers.append(row)
    
    return pd.DataFrame(selected_kmers)

if __name__ == "__main__":
    pd.set_option('display.max_columns', None)
    args = parse_arguments()

    # make "tmp" directory
    Path("./kmers/tmp/").mkdir(parents=True, exist_ok=True)


    # Load kmers.bed file
    kmerTargets = pyb.BedTool(args.scoredGuides)

        
    # Determine target file type
    targetFileType = args.targets.split(".")[-1].lower()

    if targetFileType in ["gff", "gtf", "gff3", "gff2"]:
        print(f'{args.targets} is {targetFileType.upper()} format')
        trimmed = "./kmers/tmp/trimmedAnnotation." + targetFileType
        with open(trimmed, 'w') as f:
            f.writelines(process_gtf(input_file = args.targets, feature = args.feature, percentiles = [0,args.percentile_range * 100]))
        targets = pyb.BedTool(trimmed)

        finalKmers = kmer_to_tscript(pyb.BedTool(kmerTargets).intersect(targets, wo=True))
        
    elif targetFileType in ["bed"]:
        print(f'{args.targets} is BED format')
        targets = pyb.BedTool(args.targets)
        
        finalKmers = kmer_to_bed(pyb.BedTool(kmerTargets).intersect(targets, wo=True))

        
    else:
        print(f'{args.targets} file extension unrecognized, for expected behavior pass GFF/GTF/BED file.\n \
        Defaulting to BED behavior')
        targets = pyb.BedTool(args.targets)
        
        finalKmers = kmer_to_bed(pyb.BedTool(kmerTargets).intersect(targets, wo=True))



    header = ["#chr", "start", "stop", "sequence","pam", "strand", \
                            "context", "rs3_z_score", "rs3_cdf", "rs3_score_norm",
                            "specificity", "transcript_id", "gene_id"]
    finalKmers.columns = header
    print(finalKmers.head())
    # finalKmers["id"] = finalKmers[header[3]].apply(lambda x: x.split(',')[0])
    # finalKmers.sort_values('id', inplace=True)

    # finalKmers.to_csv('./tmp/loc_filtered_guides.tsv', sep="\t", index=False)
    # finalKmers[["id,sequence,pam,chromosome,position,sense"]].to_csv('./tmp/guidscan_filtered_guides.csv', sep="\t", index=False)

    # TODO
    # score filter, print remaining guides, min median max per transcript
    # space filter print remaining guides
    # 